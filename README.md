# Advanced Lane Finding

[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)

*made by [CJ](https://github.com/vssrcj)*

---
![Final Result Gif](./result.gif)
---
Overview
---
The goals / steps of this project are the following:

* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.
* Apply a distortion correction to raw images.
* Use color transforms, gradients, etc., to create a thresholded binary image.
* Apply a perspective transform to rectify binary image ("birds-eye view").
* Detect lane pixels and fit to find the lane boundary.
* Determine the curvature of the lane and vehicle position with respect to center.
* Warp the detected lane boundaries back onto the original image.
* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.

---
### Camera calibration.

20 images of a chessboard were given in order to calibrate the camera.

This is done by finding the (9x6) corners of the chessboard, like so:
<div>
    <img src="/plots/calibration.png" height="300">
</div>

You use the collection of all of the corners to calibrate the camera, in order to undistort the image, using:
```python
   cv2.calibrateCamera(objpoints, imgpoints, img_size, None, None)
```
where *imgpoints* is the collection of the chessboard corners.

### Undistortion.

You can undistort an image, using:
```python
   cv2.undistort(img, mtx, dist, None, mtx)
```
with the result:
<div>
    <img src="/plots/undistort.png" height="300">
</div>

## Warp image.

The next step is to warp the image.  This is in order to view the image from the top.
*3D image to 2D image*.

You do this by warping the image by mapping a trapezoid area to a square area using the following points:

<table>
<tr>
   <th>Source</th> <th>Destination</th>
</tr>
<tr>
   <td>570 , 480</td> <td>300, 300</td>
</tr>
<tr>
   <td>780 , 480</td> <td>980, 300</td>
</tr>
<tr>
   <td>250 , 720</td> <td>300, 720</td>
</tr>
<tr>
   <td>1130, 270</td> <td>980, 720</td>
</tr>
</table>

<div>
    <img src="/plots/warped.png" height="300">
</div>

## Processing image.
The warped image is processed into a binary image that indicates the lane lines.
This is done by the following processes:

### 1. Gradient thresholds.
Various gradient threshold methods are used:
<div>
    <img src="/plots/gradient-thresholds.png" height="400">
</div>

The best result is retrieved where the *x-derivate* and the *y-derivate* intersects,
or the *magnitude* and the *direction* intersects.

### 2. Color spaces.
Different color spaces are used to isolate the desired channels:

**RGB**
<div>
    <img src="/plots/rgb.png" height="200">
</div>

**HLS**
<div>
    <img src="/plots/hls.png" height="200">
</div>

**HSV**
<div>
    <img src="/plots/hsv.png" height="200">
</div>

The best result is retrieved where *red* and *green* intersects, or the *saturation* and the *hue* intersects:

<div>
    <img src="/plots/color-combine.png" height="300">
</div>

### 3. Combination

A binary image is generated by combining the gradient threshold result with the color space result:
<div>
    <img src="/plots/combine.png" height="300">
</div>

### 4. Clean up image.
Finally, the image is cleaned up - all the small dots on the image are removed.  This is done for the next step, in order to better predict where the most pixels on the image lie.
<div>
    <img src="/plots/binary.png" height="300">
</div>

## Finding lane lines.
This histogram shows the amount of pixels in the y direction.  The most pixels likely corresponds to the lane lines:
<div>
    <img src="/plots/histogram.png" height="300">
</div>

### Sliding windows.
The image is divided into vertical slices.  Windows move in theses slices horizontally to check for the most amount of pixels.  Once found, a box surrounds it.
<div>
    <img src="/plots/windows.png" height="300">
</div>
Two polynomials are created that connect the boxes.  This correlates to the lane lines.

#### Subsequent images:
When a new image needs to be processed in order to detect the lines, it does not need to do a blind search, but it does so by looking for the lines within 100px of the previous polynomials.
<div>
    <img src="/plots/next-windows.png" height="300">
</div>


### Curvature and offset from center.
The polynomials retreived by the sliding windows are mapped to a real world space.
It is then easy to get the radii of curvature from the polynomials:
If the polynomial is in the form:
<div>
    <img src="/plots/polynomial.png">
</div>
And the formula of the radius of the curvature is:
<div>
    <img src="/plots/curvature-formula.png">
</div>
Then the equation to obtain curvature is:
<div>
    <img src="/plots/curvature.png">
</div>

The distance from the center of the image, is simply the distance from the mean of the beggining of the 2 lines.

These functions are implemented in `calc_curvature`.

### Fill area.
The area between the polynomials are simply colored in.  This area is then transposed onto the original image.  The curvature of the road, as well as the offset from center are also added to the image.
<div>
    <img src="/plots/fill.png" height="300">
</div>

## Pipeline.

Firstly, the camera needs to be calibrated once.
Once done, the image (frame) goes through the following pipeline.
1. Undistort the image.
2. Warp the image.
3. Combine the color space, gradient threshold, and cleanup results for a binary image.
4. If it is the first image, perform the `get_sliding_windows` function.
5. Else, perform the `get_next_sliding_windows` function.
6. Smooth the result, by using 75% of the previous polynomial and curvature, and 25% of the newly detected ones.
7. Sanitize the result, by checking if the distance between the two polynomials are too narrow.  If so, perform the initial `get_sliding_windows` function again.
8. Fill the area.

## The result.
The area between the lines are successfully colored in for the whole video: 

<a href="/result.mp4">video.mp4</a>.

The radius of the curvature of the road varies between 500m and 1000m for the turns, and >8000m for the straights, which seems acceptable.

The distance from the offset is always smalller than 0.5m, which is in bounds.

## Discussion.
### Problems.
* When the camera is mounted differently (different location or angle), then this won't work.
* The implementation is generic enough, and it may fail on different type of driving conditions.
* If the binary lines don't have a spike in the histogram to indicate the lines, then a polynomial fitting line can't be found.  *(the image cleanup helps with this)*
